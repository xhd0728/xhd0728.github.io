# üìù Publications 

> \* indicates equal contribution, and ‚Ä† indicates corresponding author.

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/paper/consrec.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# ConsRec: Denoising Sequential Recommendation through User-Consistent Preference Modeling

**Haidong Xin**$^{\*}$, [Qiushi Xiong$^{\*}$](https://scholar.google.com/citations?user=dFXhQlsAAAAJ), [Zhenghao Liu$^‚Ä†$](https://scholar.google.com/citations?user=4vrZRk0AAAAJ), [Sen Mei](https://scholar.google.com/citations?user=JWqmlrcAAAAJ), [Yukun Yan](https://scholar.google.com/citations?user=B88nSvIAAAAJ), [Shi Yu](https://scholar.google.com/citations?user=xcMVPTgAAAAJ), [Shuo Wang](https://scholar.google.com/citations?user=5vm5yAMAAAAJ), [Yu Gu](https://scholar.google.com/citations?user=IDYbTZwAAAAJ), [Ge Yu](https://scholar.google.com/citations?user=HClMOmUAAAAJ), [Chenyan Xiong](https://scholar.google.com/citations?user=E9BaEBYAAAAJ)

[**üìÉPaper**](https://arxiv.org/abs/2505.22130) \| [**üìÑPDF**](https://arxiv.org/pdf/2505.22130) \| [![](https://img.shields.io/github/stars/NEUIR/ConsRec?style=social&label=Code+Stars)](https://github.com/NEUIR/ConsRec)

- This work achieves the denoising of user interaction sequences through consistent user preference modeling, thereby improving the performance of sequential recommendation systems.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">LREC-COLING 2024</div><img src='images/paper/mmad-coling2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# MMAD: Multi-modal Movie Audio Description

[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ), [Junhao Chen](https://scholar.google.com/citations?user=uVMnzPMAAAAJ), [Xiang Li](https://scholar.google.com/citations?user=_wyYvQsAAAAJ), **Haidong Xin**, Chao Li, [Sheng Zhou$^‚Ä†$](https://scholar.google.com/citations?user=Ss76nMwAAAAJ), [Jiajun Bu](https://scholar.google.com/citations?user=OgZP2okAAAAJ)

[**üìÉPaper**](https://aclanthology.org/2024.lrec-main.998/) \| [**üìÑPDF**](https://aclanthology.org/2024.lrec-main.998.pdf) \| [**üóÇÔ∏èProject Page**](https://daria8976.github.io/mmad-page/) \| [![](https://img.shields.io/github/stars/Daria8976/MMAD?style=social&label=Code+Stars)](https://github.com/Daria8976/MMAD)

- This work has unlocked a whole new experience of watching movies for the visually impaired.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2023</div><img src='images/paper/mcm-icm-2023.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# Puzzle game: Prediction and Classification of Wordle Solution Words

**Haidong Xin**$^{\*‚Ä†}$, [Fang Wu$^{\*}$](https://wfloveiu.github.io/), [Zhitong Zhou$^{\*}$](https://scholar.google.com/citations?user=aG3jVDUAAAAJ&hl=en)

[**üìÉPaper**](https://arxiv.org/abs/2403.19433) \| [**üìÑPDF**](https://arxiv.org/pdf/2403.19433) \| [![arXiv](https://img.shields.io/badge/arXiv-2403.19433-b31b1b.svg?style=flat-square)](https://arxiv.org/abs/2403.19433)

- This work conducted a detailed numerical analysis of the Wordle game, revealing statistical patterns within it.
</div>
</div>