# ðŸ“ Publications 

> \* indicates equal contribution, and â€  indicates corresponding author.

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AI Open 2025</div><img src='images/paper/survey.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# Knowledge Intensive Agents

[Zhenghao Liu](https://scholar.google.com/citations?user=4vrZRk0AAAAJ), [Pengcheng Huang](https://scholar.google.com/citations?user=VA9mUOsAAAAJ), [Zhipeng Xu](https://scholar.google.com/citations?user=PGS_UbsAAAAJ), [Xinze Li](https://scholar.google.com/citations?user=Feo2PhwAAAAJ), [Shuliang Liu](https://scholar.google.com/citations?user=oMszRQQAAAAJ), [Chunyi Peng](https://scholar.google.com/citations?user=dZfkLg4AAAAJ), **Haidong Xin**, [Yukun Yan$^â€ $](https://scholar.google.com/citations?user=B88nSvIAAAAJ), [Shuo Wang](https://scholar.google.com/citations?user=5vm5yAMAAAAJ), [Xu Han](https://scholar.google.com/citations?user=rz4rOSMAAAAJ), [Zhiyuan Liu$^â€ $](https://scholar.google.com/citations?user=dT0v5u0AAAAJ), [Maosong Sun$^â€ $](https://scholar.google.com/citations?user=zIgT0HMAAAAJ), [Yu Gu](https://scholar.google.com/citations?user=IDYbTZwAAAAJ), [Ge Yu](https://scholar.google.com/citations?user=HClMOmUAAAAJ)

<div align="left">
<a href="https://ssrn.com/abstract=5459034"><img src="https://img.shields.io/badge/SSRN-5459034-b31b1b.svg?style=flat-square" alt="SSRN"></a>
</div>

- This work provides a comprehensive overview of Retrieval-Augmented Generation from an agentic perspective, categorizing knowledge-intensive agents into acquisition and utilization roles, and highlighting future directions for joint optimization in multi-agent RAG systems.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">WISA 2025</div><img src='images/paper/tasteplus.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# Adapting Language Models to Text Matching based Recommendation Systems

**Haidong Xin**, [Sen Mei](https://scholar.google.com/citations?user=JWqmlrcAAAAJ), [Zhenghao Liu$^â€ $](https://scholar.google.com/citations?user=4vrZRk0AAAAJ), [Xiaohua Li](http://faculty.neu.edu.cn/lixiaohua/zh_CN/index.htm), [Minghe Yu](http://faculty.neu.edu.cn/yuminghe/zh_CN/index.htm), [Yu Gu](https://scholar.google.com/citations?user=IDYbTZwAAAAJ), [Ge Yu](https://scholar.google.com/citations?user=HClMOmUAAAAJ)

<div align="left">
<a href="https://huggingface.co/xhd0728/TASTE-plus-beauty"><img src="https://img.shields.io/badge/HuggingFace-TASTE+-blue?logo=huggingface"></a>
<a href="https://github.com/NEUIR/TASTE-plus"><img src="https://img.shields.io/github/stars/NEUIR/TASTE-plus?label=stars&logo=github&color=brightgreen" alt="GitHub Repo Stars"></a>
</div>

- This work has significantly improved the performance of text matching in recommendation systems by incorporating language model pretraining.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2025</div><img src='images/paper/llmsparks.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts

[Junhao Chen](https://scholar.google.com/citations?user=uVMnzPMAAAAJ), Jingbo Sun, [Xiang Li](https://scholar.google.com/citations?user=_wyYvQsAAAAJ), **Haidong Xin**, [Yuhao Xue](https://xhyu61.github.io), Yibin Xu, [Hao Zhao$^â€ $](https://scholar.google.com/citations?hl=zh-CN&user=ygQznUQAAAAJ)

<div align="left">
<a href="https://llmsparks.github.io"><img src="https://img.shields.io/static/v1?label=Homepage&message=LLMsPark&color=blue&logo=github-pages"></a>
<a href="https://arxiv.org/abs/2509.16610"><img src="https://img.shields.io/badge/arXiv-2509.16610-b31b1b.svg?style=flat-square" alt="arXiv"></a>
</div>

- This work introduces LLMsPark, a game theory-based platform for evaluating large language models' strategic behaviors and decision-making abilities in multi-agent environments, providing a novel criterion for assessing their intelligence.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv 2025</div><img src='images/paper/consrec.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# ConsRec: Denoising Sequential Recommendation through User-Consistent Preference Modeling

**Haidong Xin**$^{\*}$, [Qiushi Xiong$^{\*}$](https://scholar.google.com/citations?user=dFXhQlsAAAAJ), [Zhenghao Liu$^â€ $](https://scholar.google.com/citations?user=4vrZRk0AAAAJ), [Sen Mei](https://scholar.google.com/citations?user=JWqmlrcAAAAJ), [Yukun Yan](https://scholar.google.com/citations?user=B88nSvIAAAAJ), [Shi Yu](https://scholar.google.com/citations?user=xcMVPTgAAAAJ), [Shuo Wang](https://scholar.google.com/citations?user=5vm5yAMAAAAJ), [Yu Gu](https://scholar.google.com/citations?user=IDYbTZwAAAAJ), [Ge Yu](https://scholar.google.com/citations?user=HClMOmUAAAAJ), [Chenyan Xiong](https://scholar.google.com/citations?user=E9BaEBYAAAAJ)

<div align="left">
<a href="https://arxiv.org/abs/2505.22130"><img src="https://img.shields.io/badge/arXiv-2505.22130-b31b1b.svg?style=flat-square" alt="arXiv"></a>
<a href="https://github.com/NEUIR/ConsRec"><img src="https://img.shields.io/github/stars/NEUIR/ConsRec?label=stars&logo=github&color=brightgreen" alt="GitHub Repo Stars"></a>
</div>

- This work improves sequential recommendation systems by extracting personalized semantic shortcuts from user-item interaction histories, enhancing the capture of stable user preferences.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">COLING 2024</div><img src='images/paper/mmad-coling2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# MMAD: Multi-modal Movie Audio Description

[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ), [Junhao Chen](https://scholar.google.com/citations?user=uVMnzPMAAAAJ), [Xiang Li](https://scholar.google.com/citations?user=_wyYvQsAAAAJ), **Haidong Xin**, Chao Li, [Sheng Zhou$^â€ $](https://scholar.google.com/citations?user=Ss76nMwAAAAJ), [Jiajun Bu](https://scholar.google.com/citations?user=OgZP2okAAAAJ)

<div align="left">
<a href="https://daria8976.github.io/mmad-page/"><img src="https://img.shields.io/static/v1?label=Homepage&message=MMAD&color=blue&logo=github-pages"></a>
<a href="https://github.com/Daria8976/MMAD"><img src="https://img.shields.io/github/stars/Daria8976/MMAD?label=stars&logo=github&color=brightgreen" alt="GitHub Repo Stars"></a>
<a href="https://aclanthology.org/2024.lrec-main.998.pdf">[ðŸ“„ Paper]</a>
</div>

- This work has unlocked a whole new experience of watching movies for the visually impaired.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">MCM/ICM 2023</div><img src='images/paper/mcm-icm-2023.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# Puzzle Game: Prediction and Classification of Wordle Solution Words

**Haidong Xin**$^{\*}$, [Fang Wu$^{\*}$](https://wfloveiu.github.io/), [Zhitong Zhou$^{\*}$](https://scholar.google.com/citations?user=aG3jVDUAAAAJ&hl=en)

<div align="left">
<a href="https://arxiv.org/abs/2403.19433"><img src="https://img.shields.io/badge/arXiv-2403.19433-b31b1b.svg?style=flat-square" alt="arXiv"></a>
</div>

- This work conducted a detailed numerical analysis of the Wordle game, revealing statistical patterns within it.
</div>
</div>