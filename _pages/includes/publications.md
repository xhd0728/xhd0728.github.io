# ðŸ“ Publications 

> \* indicates **equal contribution**, and â€  indicates **corresponding author**.

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ArXiv 2026</div><img src='images/paper/pager.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# Structured Knowledge Representation through Contextual Pages for Retrieval-Augmented Generation

*[Xinze Li](https://scholar.google.com/citations?user=Feo2PhwAAAAJ), [Zhenghao Liu$^â€ $](https://scholar.google.com/citations?user=4vrZRk0AAAAJ), *Haidong Xin**, [Yukun Yan](https://scholar.google.com/citations?user=B88nSvIAAAAJ), [Shuo Wang](https://scholar.google.com/citations?user=5vm5yAMAAAAJ), [Zheni Zeng](https://scholar.google.com/citations?user=CM3VSeQAAAAJ), [Sen Mei](https://scholar.google.com/citations?user=JWqmlrcAAAAJ), [Ge Yu](https://scholar.google.com/citations?user=HClMOmUAAAAJ), [Maosong Sun](https://scholar.google.com/citations?user=zIgT0HMAAAAJ)


<div align="left">
<a href="https://arxiv.org/abs/2601.09402"><img src="https://img.shields.io/badge/arXiv-2601.09402-b31b1b?logo=arxiv&logoColor=white" alt="arXiv"></a>
<a href="https://github.com/OpenBMB/PAGER"><img src="https://img.shields.io/github/stars/OpenBMB/PAGER?label=stars&logo=github&color=brightgreen" alt="GitHub Repo Stars"></a>
</div>

This work enhances RAG systems by constructing structured cognitive outlines to guide iterative retrieval, effectively organizing multi-dimensional knowledge into coherent pages for more accurate answer generation.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">KDD 2026</div><img src='images/paper/lisrec.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# LISRec: Modeling User Preferences with Learned Item Shortcuts for Sequential Recommendation

**Haidong Xin**, [Zhenghao Liu$^â€ $](https://scholar.google.com/citations?user=4vrZRk0AAAAJ), [Sen Mei](https://scholar.google.com/citations?user=JWqmlrcAAAAJ), [Yukun Yan](https://scholar.google.com/citations?user=B88nSvIAAAAJ), [Shi Yu](https://scholar.google.com/citations?user=xcMVPTgAAAAJ), [Shuo Wang](https://scholar.google.com/citations?user=5vm5yAMAAAAJ), [Zulong Chen](https://scholar.google.com/citations?user=nUVmSqUAAAAJ), [Yu Gu](https://scholar.google.com/citations?user=IDYbTZwAAAAJ), [Ge Yu](https://scholar.google.com/citations?user=HClMOmUAAAAJ), [Chenyan Xiong](https://scholar.google.com/citations?user=E9BaEBYAAAAJ)


<div align="left">
<a href="https://arxiv.org/abs/2505.22130"><img src="https://img.shields.io/badge/arXiv-2505.22130-b31b1b?logo=arxiv&logoColor=white" alt="arXiv"></a>
<a href="https://github.com/NEUIR/LISRec"><img src="https://img.shields.io/github/stars/NEUIR/LISRec?label=stars&logo=github&color=brightgreen" alt="GitHub Repo Stars"></a>
</div>

This work improves sequential recommendation systems by extracting personalized semantic shortcuts from user-item interaction histories, enhancing the capture of stable user preferences.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AI Open 2026 Submission</div><img src='images/paper/survey.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# Knowledge Intensive Agents

[Zhenghao Liu](https://scholar.google.com/citations?user=4vrZRk0AAAAJ), [Pengcheng Huang](https://scholar.google.com/citations?user=VA9mUOsAAAAJ), [Zhipeng Xu](https://scholar.google.com/citations?user=PGS_UbsAAAAJ), [Xinze Li](https://scholar.google.com/citations?user=Feo2PhwAAAAJ), [Shuliang Liu](https://scholar.google.com/citations?user=oMszRQQAAAAJ), [Chunyi Peng](https://scholar.google.com/citations?user=dZfkLg4AAAAJ), **Haidong Xin**, [Yukun Yan$^â€ $](https://scholar.google.com/citations?user=B88nSvIAAAAJ), [Shuo Wang](https://scholar.google.com/citations?user=5vm5yAMAAAAJ), [Xu Han](https://scholar.google.com/citations?user=rz4rOSMAAAAJ), [Zhiyuan Liu$^â€ $](https://scholar.google.com/citations?user=dT0v5u0AAAAJ), [Maosong Sun$^â€ $](https://scholar.google.com/citations?user=zIgT0HMAAAAJ), [Yu Gu](https://scholar.google.com/citations?user=IDYbTZwAAAAJ), [Ge Yu](https://scholar.google.com/citations?user=HClMOmUAAAAJ)

  
<div align="left">
<a href="https://ssrn.com/abstract=5459034"><img src="https://img.shields.io/badge/SSRN-5459034-b31b1b?logo=arxiv&logoColor=white" alt="SSRN"></a>
</div>

This work provides a comprehensive overview of Retrieval-Augmented Generation from an agentic perspective, categorizing knowledge-intensive agents into acquisition and utilization roles, and highlighting future directions for joint optimization in multi-agent RAG systems.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">WISA 2025</div><img src='images/paper/tasteplus.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# Adapting Language Models to Text Matching based Recommendation Systems

**Haidong Xin**, [Sen Mei](https://scholar.google.com/citations?user=JWqmlrcAAAAJ), [Zhenghao Liu$^â€ $](https://scholar.google.com/citations?user=4vrZRk0AAAAJ), [Xiaohua Li](http://faculty.neu.edu.cn/lixiaohua/zh_CN/index.htm), [Minghe Yu](http://faculty.neu.edu.cn/yuminghe/zh_CN/index.htm), [Yu Gu](https://scholar.google.com/citations?user=IDYbTZwAAAAJ), [Ge Yu](https://scholar.google.com/citations?user=HClMOmUAAAAJ)

  
<div align="left">
<a href="https://huggingface.co/xhd0728/TASTE-plus-beauty"><img src="https://img.shields.io/badge/HuggingFace-TASTE+-blue?logo=huggingface"></a>
<a href="https://github.com/NEUIR/TASTE-plus"><img src="https://img.shields.io/github/stars/NEUIR/TASTE-plus?label=stars&logo=github&color=brightgreen" alt="GitHub Repo Stars"></a>
</div>

This work has significantly improved the performance of text matching in recommendation systems by incorporating language model pretraining.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">EMNLP 2025</div><img src='images/paper/llmsparks.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts

[Junhao Chen](https://scholar.google.com/citations?user=uVMnzPMAAAAJ), Jingbo Sun, [Xiang Li](https://scholar.google.com/citations?user=_wyYvQsAAAAJ), **Haidong Xin**, [Yuhao Xue](https://xhyu61.github.io), Yibin Xu, [Hao Zhao$^â€ $](https://scholar.google.com/citations?hl=zh-CN&user=ygQznUQAAAAJ)

  
<div align="left">
<a href="https://llmsparks.github.io"><img src="https://img.shields.io/static/v1?label=Homepage&message=LLMsPark&color=blue&logo=github-pages"></a>
<a href="https://arxiv.org/abs/2509.16610"><img src="https://img.shields.io/badge/arXiv-2509.16610-b31b1b?logo=arxiv&logoColor=white" alt="arXiv"></a>
</div>

This work introduces LLMsPark, a game theory-based platform for evaluating large language models' strategic behaviors and decision-making abilities in multi-agent environments, providing a novel criterion for assessing their intelligence.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">COLING 2024</div><img src='images/paper/mmad-coling2024.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# MMAD: Multi-modal Movie Audio Description

[Xiaojun Ye](https://scholar.google.com/citations?user=BKMYsm4AAAAJ), [Junhao Chen](https://scholar.google.com/citations?user=uVMnzPMAAAAJ), [Xiang Li](https://scholar.google.com/citations?user=_wyYvQsAAAAJ), **Haidong Xin**, Chao Li, [Sheng Zhou$^â€ $](https://scholar.google.com/citations?user=Ss76nMwAAAAJ), [Jiajun Bu](https://scholar.google.com/citations?user=OgZP2okAAAAJ)

  
<div align="left">
<a href="https://daria8976.github.io/mmad-page/"><img src="https://img.shields.io/static/v1?label=Homepage&message=MMAD&color=blue&logo=github-pages"></a>
<a href="https://github.com/Daria8976/MMAD"><img src="https://img.shields.io/github/stars/Daria8976/MMAD?label=stars&logo=github&color=brightgreen" alt="GitHub Repo Stars"></a>
<a href="https://aclanthology.org/2024.lrec-main.998.pdf">[ðŸ“„ Paper]</a>
</div>

This work has unlocked a whole new experience of watching movies for the visually impaired.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">MCM/ICM 2023</div><img src='images/paper/mcm-icm-2023.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

# Puzzle Game: Prediction and Classification of Wordle Solution Words

**Haidong Xin**$^{\*â€ }$, [Fang Wu$^{\*}$](https://wfloveiu.github.io/), [Zhitong Zhou$^{\*}$](https://scholar.google.com/citations?user=aG3jVDUAAAAJ&hl=en)

  
<div align="left">
<a href="https://arxiv.org/abs/2403.19433"><img src="https://img.shields.io/badge/arXiv-2403.19433-b31b1b?logo=arxiv&logoColor=white" alt="arXiv"></a>
</div>

This work conducted a detailed numerical analysis of the Wordle game, revealing statistical patterns within it.
</div>
</div>